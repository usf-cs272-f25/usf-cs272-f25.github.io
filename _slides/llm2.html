---
title: LLM Internals
layout: raw
---

# LLM Internals

---

## Agenda

1. Perceptron <!-- .element: class="fragment" -->
1. Neural Networks <!-- .element: class="fragment" -->
1. Training <!-- .element: class="fragment" -->
1. Inference <!-- .element: class="fragment" -->
1. Hardware <!-- .element: class="fragment" -->
1. Philosohical Questions <!-- .element: class="fragment" -->

---

# Perceptron

1. Invented by Frank Rosenblatt in 1958 <!-- .element: class="fragment" -->
1. Goal: a machine to classify military surveillance images <!-- .element: class="fragment" -->
1. Implemented as a simple model of a biological neuron <!-- .element: class="fragment" -->
1. Inputs multiplied by weights, summed, passed through an activation function <!-- .element: class="fragment" -->

---

# Neural Networks

1. Start with input values <!-- .element: class="fragment" -->
1. Build a "hidden layer" of nodes connected by weights <!-- .element: class="fragment" -->
1. Each node applies an activation function to the weighted sum (dot product) of its inputs <!-- .element: class="fragment" -->
1. Output layer produces final results (inference) <!-- .element: class="fragment" -->

---

# Types of Neural Networks

1. Convolutional Neural Networks (CNNs) for image processing <!-- .element: class="fragment" -->
1. Recurrent Neural Networks (RNNs) for sequential data <!-- .element: class="fragment" -->
1. Long Short-Term Memory (LSTM) networks for long-range dependencies <!-- .element: class="fragment" -->
1. Transformers for attention-based processing <!-- .element: class="fragment" -->

---

# Transformer Model

1. _Attention Is All You Need_ introduced the Transformer model <!-- .element: class="fragment" -->
1. Self attention mechanism intended to translate natural language <!-- .element: class="fragment" -->
1. Uses "heads" to focus on different parts of the input sequence <!-- .element: class="fragment" -->
1. Parallel processing of input data enables hardware acceleration <!-- .element: class="fragment" -->

---

# Training The Model

1. Using large datasets of text (e.g., Common Crawl, Wikipedia) <!-- .element: class="fragment" -->
1. Tokenizing text into subword units or words <!-- .element: class="fragment" -->
1. Project tokens into embeddings using matrix multiplication <!-- .element: class="fragment" -->
1. Adjust node weights using feed-forward propagation of weights <!-- .element: class="fragment" -->

---

# Training Part II

1. Measure how good the output is using a loss function <!-- .element: class="fragment" -->
1. Calculate the gradient of the loss function with respect to each weight <!-- .element: class="fragment" -->
1. Use back-propagation to update weights in the opposite direction of the gradient <!-- .element: class="fragment" -->
1. Repeat over many epochs until the model converges <!-- .element: class="fragment" -->

---

# Training Part III

1. Update weights (same math) using supervised learning <!-- .element: class="fragment" -->
1. Use labeled data and supervised fine-tuning for specific tasks <!-- .element: class="fragment" -->
1. "Alignment" with human values using Reinforcement Learning from Human Feedback (RLHF) <!-- .element: class="fragment" -->
---
# Inference

1. Words -> tokens -> vectors <!-- .element: class="fragment" -->
1. Activate the neural network by weights<!-- .element: class="fragment" -->
1. Infer the next token from the probability distribution<!-- .element: class="fragment" -->
1. Repeat for each token in the output sequence ("generative")<!-- .element: class="fragment" -->
---

# Math For LLMs

1. ReLU activation function: f(x) = max(0, x) <!-- .element: class="fragment" -->
1. Softmax function takes raw scores ("logits"), generates a probability distribution using exponentials <!-- .element: class="fragment" -->
1. Gradients are calculated using partial derivatives <!-- .element: class="fragment" -->
1. Optimizers adjust weights so minimize the gradient of the loss function <!-- .element: class="fragment" -->

---

# Key Concepts

1. Self-attention is both effective and easily accelerated in hardware <!-- .element: class="fragment" -->
1. Gradient descent and back-propagation are key to training <!-- .element: class="fragment" -->
1. Large datasets and compute power are required for good results <!-- .element: class="fragment" -->

---

# Hardware

1. GPU/TPU: parallel matrix multiplication of floating point numbers <!-- .element: class="fragment" -->
1. High memory bandwidth for large models <!-- .element: class="fragment" -->
1. NVIDIA GPUs are optimized for this work, but the math is not magic <!-- .element: class="fragment" -->
1. Data centers focused on GPU/TPU machines draw enormous power <!-- .element: class="fragment" -->

---

# Limitations

1. Models can be poisoned with incorrect or malicious training data <!-- .element: class="fragment" -->
1. Transformer does not support continuous learning <!-- .element: class="fragment" -->

---

# Philosophical Issues

1. Is this intelligence? <!-- .element: class="fragment" -->
1. We (still) don't have a rigorous theory of intelligence <!-- .element: class="fragment" -->
1. To the extent that the model was trained on correct and unbiased information, intelligence (whatever it is) might be said to be encoded in the weights of the model <!-- .element: class="fragment" -->
1. Is "superintelligence" possible with this approach? <!-- .element: class="fragment" -->

---

# Your Reflections

1. There is no magic, only Linear Algebra with a dash of Multivariable Calculus <!-- .element: class="fragment" -->
1. Are you surprised or disappointed? <!-- .element: class="fragment" -->
